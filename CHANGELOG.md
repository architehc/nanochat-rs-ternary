# Changelog

All notable changes to nanochat-rs-ternary will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).

## [Unreleased]

### ðŸŽ¯ Next Steps
- Retrain model with entropy regularization (in progress)
- Achieve >80% compilation success rate
- Export to GGUF format for deployment
- Add HumanEval-Rust benchmark suite
- Implement NUMA-aware inference server

## [2026-02-10] - Model Collapse Fix

### ðŸ› Fixed
- **Critical**: Model collapse due to softmax temperature explosion
  - Logits reached ~250 (should be -10 to +10)
  - Model generated only repeated `{` tokens despite low loss (1.5)
  - Root cause: No entropy regularization in loss function

### âœ¨ Added
- **Entropy regularization**: `loss = ce_loss - 0.01 * entropy(softmax(logits))`
- **Enhanced logging**: Now tracks `ce`, `H` (entropy), alongside `loss`
- **StepStats expansion**: Added `ce_loss` and `entropy` fields
- **Diagnostic guide**: Added section in README on monitoring entropy

### ðŸ” Analysis
- Verified training data is balanced (token 1391 only 0.87%)
- Confirmed model architecture has proper RMSNorm
- Identified missing regularization as root cause
- Expected entropy range: 6-8 for healthy models

### ðŸ“Š Commits
- `2f57c70`: Fix model collapse with entropy regularization

## [2026-02-10] - Comprehensive Benchmarking System

### âœ¨ Added
- **Model benchmarking framework** (`nanochat-eval`)
  - 16 diverse Rust test prompts
  - Compilation success rate measurement via `rustc --error-format=json`
  - AST parsing with `syn` crate for structural metrics
  - Code quality metrics: lines, functions, cyclomatic complexity
  - Performance tracking: tokens/sec, generation latency
  - JSON output for longitudinal tracking

### ðŸ“ Documentation
- Created `BENCHMARK_README.md` with detailed usage guide
- Example benchmark results and interpretation
- Metrics explanation and target values

### ðŸ“Š Commits
- `a0b2fbb`: Add comprehensive model benchmarking system

## [2026-02-09] - Training Infrastructure Improvements

### ðŸ› Fixed
- **Data inspection tool bug**: Was reading u16 instead of u32 tokens
  - Caused false diagnosis of data corruption (50% token 0)
  - Actual data is healthy: 4.2M tokens of real Rust code
- **Checkpoint loading**: Training now properly resumes from last step
  - Fixed `Trainer::from_checkpoint()` implementation
  - Correctly restores optimizer state and global_step

### âœ¨ Added
- **Training monitor script**: Real-time dashboard (`scripts/monitor_training.sh`)
  - Shows latest progress, loss trends, checkpoint status
  - Detects issues (OOM, errors, killed processes)
  - Displays remaining steps and progress percentage

### ðŸ”§ Improved
- **OOM mitigation**: Reduced batch_size=1, seq_len=256
  - Candle memory leak workaround
  - More frequent checkpointing (every 500 steps)
- **Accelerated training**: 10K steps (was 30K) with higher LR

### ðŸ“Š Commits
- `6cac2a5`: Fix data inspection tool and add training monitor
- `d67e3da`: Fix checkpoint loading to properly resume training

## [2026-02-08] - MaxRL Training Pipeline

### âœ¨ Added
- **MaxRL (Maximum Likelihood RL)**: 20x more efficient than GRPO
  - Only learns from correct samples (reward > threshold)
  - No baseline estimation needed
  - Simpler implementation, better convergence
- **Complete RL infrastructure**:
  - `train_maxrl.sh`: Standalone MaxRL training script
  - `train_pipeline_accelerated.sh`: End-to-end automation
  - Integration with compiler feedback
- **Training configurations**:
  - `train_stable_v2.sh`: Optimized supervised learning
  - Warmup-Stable-Decay (WSD) learning rate schedule
  - Gradient clipping and weight decay

### ðŸ“Š Commits
- `c05d5c3`: Add complete MaxRL training pipeline
- `638d16a`: Add MaxRL for 20x better training efficiency
- `e3db906`: Add production-grade training pipeline

## [2026-02-07] - GRPO/GSPO RL System

### âœ¨ Added
- **Reinforcement learning infrastructure**:
  - GRPO (Group Relative Policy Optimization)
  - GSPO (Group Standardized Policy Optimization)
  - Compiler feedback integration via `rustc --error-format=json`
  - Reward computation from compilation success + AST quality
- **AST analysis**:
  - Structural metrics: function count, complexity
  - Dead code detection
  - Pattern matching coverage
- **RL trainer**:
  - Policy gradient with baseline estimation
  - Advantage normalization
  - Multiple samples per prompt

### ðŸ“Š Commits
- `f47c9a5`: Add RL evaluation demo and results
- `cc69070`: Add GRPO/GSPO reinforcement learning system

## [2026-02-06] - Core Training Infrastructure

### âœ¨ Added
- **Rust native training** with Candle ML framework
  - Muon optimizer for linear weights (lr=0.02)
  - Lion optimizer for mHC/norms/embeddings (lr=1e-4)
  - Gradient clipping and weight decay
  - Checkpoint saving and loading
- **Training data**: 4.2M tokens of Rust code
  - Tokenized with GPT-2 tokenizer
  - Binary format (u32 little-endian)
  - Includes: functions, structs, async, traits, macros

### ðŸ”§ Workarounds
- **Candle memory leak**: Batch size reduction, frequent checkpoints
- **CUDA OOM**: Conservative memory planning

### ðŸ“Š Commits
- `d2a4c5b`: Add Rust code training infrastructure

## [2026-02-05] - Shakespeare Training Showcase

### âœ¨ Added
- **Proof of concept**: Character-level Shakespeare generation
- **Synthetic code patterns**: Demonstration of model capabilities
- **Comprehensive documentation**: Training guides and results

### ðŸ“Š Commits
- `059bcfd`: Add actual Shakespeare test results
- `70d5088`: Add comprehensive Shakespeare showcase
- `fa12b69`: Add Shakespeare training showcase with bug fixes
- `280b9e7`: Add training data guide

## [2026-02-04] - Advanced Training Features

### âœ¨ Added
- **MoE (Mixture of Experts)**: 8 experts, top-2 routing
- **DeltaNet attention**: Alternative to standard MHA
- **Batched prefill**: Multi-token parallel processing
- **Weight tying**: Share tok_embed and lm_head weights
- **Ternary kernel integration**: GEMV dispatch to optimized kernels

### ðŸ“Š Commits
- `11f9509`: Implement all 4 advanced training features

## [2026-02-03] - Foundation Complete

### âœ… Phase Completion

All 6 phases of CLAUDE.md implementation plan complete:

1. **Phase 1: ternary-core** (95 tests)
   - Bit-exact packing and unpacking
   - Planar SoA layout with 128-byte alignment
   - GGUF I/O with custom Q1_58 type
   - NUMA allocation and huge pages support

2. **Phase 2: ternary-kernels** (8 tests)
   - C kernel FFI with v3.3.1 VPERMW/VPERMB
   - AVX2 PSHUFB kernel: 14-31 GOPS
   - CUDA dp4a decode kernel
   - Triangle of Truth validation

3. **Phase 3: mhc-lite** (42 tests)
   - Exact Birkhoff-von Neumann decomposition
   - N=2: Single-parameter identityâ†”swap
   - N=4: Full 24-permutation Sâ‚„ group
   - Composite gain â‰¤ 1.0 guarantee

4. **Phase 4: nanochat-model** (62 tests)
   - Full transformer with ternary BitLinear
   - mHC routing in residual connections
   - MHA/GQA/MLA attention variants
   - SwiGLU FFN with optional MoE

5. **Phase 5: nanochat-train** (57 tests)
   - Rust native training (Python removed)
   - Muon + Lion optimizer split
   - WSD learning rate schedule
   - Checkpointing and resume

6. **Phase 6: nanochat-serve** (36 tests)
   - HTTP inference server with SSE streaming
   - KV-cache for efficient decode
   - NUMA-aware thread pools
   - Error handling and chat UI

### ðŸŽ‰ Integration Tests (49 passing)
- Triangle of Truth: All kernel paths match
- mHC property tests: Doubly stochastic verified
- Roundtrip: Packâ†’GGUFâ†’loadâ†’GEMVâ†’verify
- E2E generate: Full model forward pass
- Cross-validation: Rustâ†”Python agreement

### ðŸ“Š Final Stats
- **349 tests passing**
- **0 clippy warnings**
- **AVX2 kernel: 14-31 GOPS**
- **Model size: 127M params (nano-125M)**

---

## Version History

- **v0.3.0** (2026-02-10): Model collapse fix + comprehensive benchmarking
- **v0.2.0** (2026-02-09): Training infrastructure + MaxRL pipeline
- **v0.1.0** (2026-02-03): Core implementation complete (all 6 phases)

## Contributors

Built with assistance from Claude Opus 4.6 via Claude Code.

Special thanks to:
- Microsoft Research (BitNet architecture)
- bitnet.cpp project (kernel optimizations)
- Candle ML framework (Rust training infrastructure)
