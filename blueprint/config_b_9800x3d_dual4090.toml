# Config B: Ryzen 9800X3D + 2Ã— RTX 4090
# Training Configuration for Fast GPU Training with Tensor Parallelism

[model]
name = "nanochat-3b-looplm"
architecture = "LoopLM"
dim = 1536
n_layers = 20
n_heads = 12
vocab_size = 50257
max_seq_len = 4096
group_size = 128
mhc_n_streams = 2
n_loops = 4

[training]
batch_size = 4  # 2 per GPU
seq_len = 4096
total_steps = 150000
warmup_steps = 1000
stable_steps = 120000
decay_steps = 29000
peak_lr = 0.0004
min_lr = 0.00004
muon_lr = 0.025
lion_lr = 0.00012
entropy_weight = 0.08
max_loop_depth = 4
grad_clip = 4.0
accumulation_steps = 8

[training.optimizer]
type = "Hybrid"
muon_momentum = 0.95
lion_beta1 = 0.9
lion_beta2 = 0.99
weight_decay = 0.01

[data]
num_workers = 8
prefetch_factor = 4
pin_memory = true
verify_every_n_examples = 50
min_compile_rate = 0.88

[data.mix]
high_quality_rust = 0.50
verified_examples = 0.30
synthetic_data = 0.15
documentation = 0.05

[distributed]
world_size = 2
tensor_parallel = true
pipeline_parallel = false
backend = "nccl"
init_method = "env://"
bucket_size_mb = 25
allreduce_bucket_size = 500000000

[hardware]
numa_aware = false
cpu_threads = 16
interop_threads = 4
gpu_memory_fraction = 0.92
mixed_precision = true
gradient_checkpointing = true
tensor_parallel_size = 2
preferred_kernel = "CUDA"
cuda_math_mode = "fast"

[hardware.cuda]
enable_tf32 = true
enable_fp16 = true
benchmark = true
deterministic = false

[checkpointing]
interval = 2000
keep_last_n = 3
save_optimizer_state = true
async_save = true

[evaluation]
eval_every = 10000
metrics = ["loss", "perplexity", "compile_rate", "humaneval_pass@1"]
n_eval_examples = 500

[logging]
level = "INFO"
log_file = "training_config_b.log"
use_tensorboard = true
tensorboard_dir = "runs/config_b"
