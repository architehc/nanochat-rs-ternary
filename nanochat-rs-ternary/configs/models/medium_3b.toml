# Medium 3B - High-quality code generation with MoE
# Training time: ~24 hours on single GPU
# Memory: ~48GB GPU
# Use case: Production Rust codegen, competitive with GPT-3.5 on code

[model]
name = "nanochat-medium-3b"
vocab_size = 32000
hidden_dim = 2048
num_layers = 28
num_heads = 32
num_kv_heads = 8
ffn_mult = 3.5
max_seq_len = 8192
group_size = 128
mhc_n_streams = 4  # N=4 for better stability at scale

use_bitlinear = true
activation_bits = 8

# Hybrid attention: 70% MHA, 30% DeltaNet
deltanet_ratio = 0.3

# MoE for efficiency
use_moe = true
num_experts = 8
num_active_experts = 2
expert_capacity_factor = 1.25

[training]
total_steps = 200000
batch_size = 32
seq_length = 8192
learning_rate = 0.01
warmup_steps = 5000
weight_decay = 0.01
grad_clip = 1.0
entropy_weight = 0.01

# E3 optimizations
use_mtp = true
mtp_n_tokens = 4
mtp_weight = 0.2

use_collider = true
collider_threshold = 0.3
collider_sparsity = 0.35

use_async_loader = true
num_workers = 8
prefetch_batches = 16

# Hybrid optimizer
optimizer_type = "muon"
muon_momentum = 0.95
lion_lr_mult = 0.003
lion_weight_decay = 0.1

# 8-bit Muon for memory efficiency
use_8bit_optim = true

schedule_type = "wsd"
stable_ratio = 0.8
min_lr_ratio = 0.1

[hardware]
target = "single_gpu"
fp16 = true
gradient_checkpointing = true
flash_attention = true
