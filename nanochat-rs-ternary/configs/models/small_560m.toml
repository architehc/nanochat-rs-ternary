# Small 560M (d20 architecture) - Production-ready code generation
# Training time: ~8 hours on single GPU
# Memory: ~16GB GPU
# Use case: Rust code completion, small-scale deployment

[model]
name = "nanochat-small-560m"
vocab_size = 32000
hidden_dim = 1024
num_layers = 20
num_heads = 16
num_kv_heads = 4
ffn_mult = 3.5
max_seq_len = 4096
group_size = 128
mhc_n_streams = 2

use_bitlinear = true
activation_bits = 8

# Hybrid attention: 80% MHA, 20% DeltaNet for long context
deltanet_ratio = 0.2

[training]
total_steps = 100000
batch_size = 16
seq_length = 4096
learning_rate = 0.015
warmup_steps = 3000
weight_decay = 0.01
grad_clip = 1.0
entropy_weight = 0.01

# E3 optimizations
use_mtp = true
mtp_n_future_tokens = 4
use_async_loader = true
num_workers = 6
prefetch_batches = 12

optimizer_type = "muon"
muon_momentum = 0.95
lion_lr_mult = 0.005
lion_weight_decay = 0.1

schedule_type = "wsd"
stable_ratio = 0.8
min_lr_ratio = 0.1

[hardware]
target = "single_gpu"
fp16 = true
gradient_checkpointing = true
