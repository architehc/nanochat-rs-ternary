# Nano 125M - Fast iteration, debugging, local testing
# Training time: ~2 hours on single GPU
# Memory: ~4GB GPU
# Use case: Development, quick experiments, CI/CD testing

[model]
name = "nanochat-nano-125m"
vocab_size = 32000
hidden_dim = 768
num_layers = 12
num_heads = 12
num_kv_heads = 4  # GQA for efficiency
ffn_mult = 3.5
max_seq_len = 2048
group_size = 128
mhc_n_streams = 2

# Ternary quantization
use_bitlinear = true
activation_bits = 8

[training]
total_steps = 50000
batch_size = 8
seq_length = 2048
learning_rate = 0.02
warmup_steps = 2000
weight_decay = 0.01
grad_clip = 1.0
entropy_weight = 0.01

# E3 optimizations
use_mtp = true
mtp_n_future_tokens = 4
use_async_loader = true
num_workers = 4
prefetch_batches = 8

optimizer_type = "muon"
muon_momentum = 0.95
lion_lr_mult = 0.005
lion_weight_decay = 0.1

schedule_type = "wsd"
stable_ratio = 0.8
min_lr_ratio = 0.1

[hardware]
target = "single_gpu"
fp16 = true
gradient_checkpointing = false
