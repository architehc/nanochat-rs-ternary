# Test configuration for Async Data Loader
# Expected: 90%+ GPU utilization, 30-50% training speedup

[model]
dim = 512
n_layers = 8
n_heads = 8
n_kv_heads = 2
ffn_mult = 2.67
vocab_size = 4096
max_seq_len = 1024
group_size = 128
mhc_n_streams = 2

[training]
batch_size = 16
total_steps = 1000
log_interval = 50
checkpoint_interval = 500
keep_last_checkpoints = 2

# Async Data Loader (90%+ GPU utilization)
# Multi-threaded prefetching eliminates data loading bottleneck
use_async_loader = true
async_n_workers = 4          # 4 preprocessing threads (parallel tokenization/augmentation)
async_prefetch_size = 8      # Prefetch 8 batches ahead of GPU consumption

# Standard optimizer settings
use_8bit_optim = false
use_galore = false
muon_lr = 0.02
muon_momentum = 0.95
lion_lr = 0.0001
warmup_steps = 100
gradient_clip_norm = 1.0

# mHC architecture
use_mhc = true

# Regularization
entropy_weight = 0.01

# Expected Benefits:
# - GPU utilization: 90-95% (vs 60-70% with sync loader)
# - Training throughput: +30-50% more steps/second
# - Memory overhead: ~1-2% (8 prefetch × batch_memory)
# - CPU usage: 4 workers × ~25% = 100% (one core fully utilized)

# Hyperparameter Guidelines:
# - async_n_workers: Start with 4, increase if preprocessing is slow
#   - Small datasets (in-memory): 2-4 workers
#   - Large datasets (disk I/O): 4-8 workers
#   - Heavy preprocessing (tokenization): 6-8 workers
# - async_prefetch_size: Start with 8, increase for variable batch times
#   - Fast GPU (Blackwell): 12-16 for deeper queue
#   - Slow GPU or CPU training: 4-6 sufficient
#   - Memory constrained: Reduce to 4

# Hardware-Specific Recommendations:
# Config A (Threadripper + Blackwell):
#   async_n_workers = 8
#   async_prefetch_size = 16
# Config B (9800X3D + Dual 4090):
#   async_n_workers = 4
#   async_prefetch_size = 8
# Config C (Dual EPYC + RTX PRO 6000):
#   async_n_workers = 12
#   async_prefetch_size = 8
