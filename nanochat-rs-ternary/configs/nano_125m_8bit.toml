# nano-125m with 8-bit Quantized Optimizer
# ~75% optimizer memory reduction
# Hardware: Any (Dual EPYC, GPU, etc.)

# Model architecture
dim = 768
n_layers = 12
n_heads = 12
n_kv_heads = 12
ffn_mult = 2.6875
vocab_size = 50257
max_seq_len = 512
group_size = 128
mhc_n_streams = 2
weight_tied = true
rope_theta = 10000.0

# Training hyperparams
lr = 0.02
mhc_lr = 0.0001
weight_decay = 0.0
batch_size = 8
grad_accum_steps = 4
warmup_steps = 500
total_steps = 50000
decay_start_frac = 0.8
grad_clip = 1.0
ns_steps = 5
muon_momentum = 0.95
lion_betas = [0.9, 0.99]

# E2 Optimizer Enhancements
use_8bit_optim = true      # Enable 8-bit quantized optimizer states
use_galore = false         # GaLore disabled
galore_rank = 256          # Not used when use_galore=false
galore_update_freq = 200   # Not used when use_galore=false

# Distillation (optional)
distill_kl_weight = 0.0
loop_scale_penalty = 0.0
