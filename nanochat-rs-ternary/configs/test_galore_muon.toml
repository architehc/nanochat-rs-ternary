# Test configuration for GaLore 2 + 8-bit Muon integration
# Expected: 50-65% memory reduction vs baseline

[model]
dim = 512
n_layers = 8
n_heads = 8
n_kv_heads = 2
ffn_mult = 2.67
vocab_size = 4096
max_seq_len = 1024
group_size = 128
mhc_n_streams = 2

[training]
batch_size = 4
seq_len = 512
total_steps = 500
log_interval = 50
checkpoint_interval = 500
keep_last_checkpoints = 2

# 8-bit quantized Muon optimizer (arXiv:2509.23106)
use_8bit_optim = true
muon_lr = 0.02
muon_momentum = 0.95
muon_nesterov = true
muon_lr_mult = 1.0

# GaLore 2 low-rank gradient projection (arXiv:2504.20437)
use_galore = true
galore_rank = 128
galore_update_freq = 200

# Lion for embeddings + norms
lion_lr = 0.0001
lion_beta1 = 0.9
lion_beta2 = 0.99
lion_weight_decay = 0.1

# WSD schedule
warmup_steps = 100
stable_steps = 300
min_lr_ratio = 0.1

# Ternary quantization
use_ternary_qat = true

# mHC architecture
use_mhc = true

# Regularization
entropy_weight = 0.01
gradient_clip_norm = 1.0
