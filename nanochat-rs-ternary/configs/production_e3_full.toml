# Production Training Config: d20 (560M) with Full E3 Stack
#
# This configuration enables ALL E3 optimizations for maximum throughput:
# - Multi-Token Prediction (MTP): Denser training signal
# - Collider Token Filtering: 35% faster backprop
# - Async Data Loader: 90%+ GPU utilization
# - mHC Analysis: Adaptive initialization + health monitoring
# - FIRE Reinitialization: Continual learning capability
# - Training-Free GRPO: Experience library for RL (manual integration)

# Model architecture: d20 (560M parameters)
dim = 768
n_layers = 24
n_heads = 12
n_kv_heads = 12  # MHA for d20
ffn_mult = 3.5
vocab_size = 50257  # GPT-2 tokenizer
max_seq_len = 2048
group_size = 128
mhc_n_streams = 2
weight_tied = false
rope_theta = 10000.0

# Training hyperparameters (optimized for RTX PRO 6000 Ada 96GB)
lr = 0.02              # Muon LR for weight matrices
mhc_lr = 1e-4          # Lion LR for mHC parameters
weight_decay = 0.0     # No weight decay for Muon
batch_size = 8         # 8 × 2048 = 16K tokens/batch
grad_accum_steps = 4   # Effective batch = 32 × 2048 = 65K tokens
warmup_steps = 2000    # WSD schedule: 2K warmup
total_steps = 50000    # 50K steps × 65K = 3.25B tokens
decay_start_frac = 0.8 # Decay starts at 40K steps
grad_clip = 1.0
ns_steps = 3           # Newton-Schulz iterations for Muon
muon_momentum = 0.95
lion_betas = [0.9, 0.99]

# E2 Advanced Optimizers (disabled for baseline comparison)
use_8bit_optim = false
use_galore = false
galore_rank = 256
galore_update_freq = 200

# E3 P0: Multi-Token Prediction (ENABLED)
use_mtp = true
mtp_n_tokens = 3       # Predict 3 future tokens (aggressive)
mtp_weight = 0.2       # 20% auxiliary loss weight

# E3 P1: Collider Token Filtering (ENABLED)
use_collider = true
collider_threshold = 0.3   # Importance threshold
collider_sparsity = 0.35   # Target 35% sparsity (35% faster backprop)

# E3 P0: Async Data Loader (ENABLED)
use_async_loader = true
async_n_workers = 6        # 6 worker threads for preprocessing
async_prefetch_size = 12   # Prefetch 12 batches ahead

# Stage-1 distillation (disabled for baseline)
# For LoopLM training, set distill_teacher and loop_scale_penalty
distill_teacher = ""
distill_kl_weight = 0.0
loop_scale_penalty = 0.0
