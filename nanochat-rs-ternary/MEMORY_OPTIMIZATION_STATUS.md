# Memory Optimization Techniques - Status Report

## ‚úÖ Implemented: 8-bit Quantized Muon Optimizer

### What It Does
**8-bit optimizer state quantization** - Stores momentum buffers in INT8 instead of FP32

**Implementation**: `crates/nanochat-train/src/optim/muon_quantized.rs`

**Memory Savings**: ~75% reduction in optimizer memory
- FP32 momentum: 4 bytes per parameter
- INT8 momentum: 1 byte per parameter + per-block scale (128 block size)
- Effective: 1.03 bytes/param vs 4 bytes/param = **74% reduction**

**Technique**: Per-block absmax quantization
```
For each 128-element block:
  absmax = max(|values|)
  scale = absmax / 127
  quantized = round(value / scale).clamp(-127, 127) as i8
```

### Status
- ‚úÖ Code implemented (muon_quantized.rs)
- ‚úÖ Config flag exists (`use_8bit_optim`)
- ‚úÖ **TESTED** - Benchmark confirms optimizer variant active
- ‚úÖ **VALIDATED** - 74.2% memory reduction confirmed (10MB ‚Üí 2MB for test model)

### How to Enable
```rust
// In config
use_8bit_optim: true  // Enable 8-bit optimizer states
```

Currently enabled in:
- `test-8bit` config (validated via benchmark)
- `medium_3b` config (3B parameter model)

### Benchmark Results (Feb 15, 2026)

**Test Configuration:**
- Model: test-8bit (28.4M params)
- Optimizer states: FP32 10MB ‚Üí INT8 2MB
- **Memory reduction: 74.2%** ‚úÖ
- Throughput: 3396 tok/s vs 3490 tok/s baseline (-2.7%, negligible)
- Training: Stable, no NaN/Inf issues

**Projected Savings for d20-e3-full (282M params):**
- FP32 optimizer: ~2.2 GB
- INT8 optimizer: ~0.3 GB
- **Savings: ~1.9 GB**

See `benchmark_results_8bit_*/REPORT.md` for full analysis.

## ‚ùå NOT Implemented: FP8 Training

### What FP8 Would Do
**FP8 activations/gradients** - Store intermediate values in 8-bit float instead of FP16/FP32

**Potential Memory Savings**: 50-75% activation memory
- FP32 activations: 4 bytes per element
- FP16 activations: 2 bytes per element  
- FP8 activations: 1 byte per element

**NOT IMPLEMENTED**: 
- No FP8 dtype in Candle framework
- No FP8 tensor operations
- No FP8 mixed precision training

### Why Not FP8?
1. **Candle limitation**: No native FP8 support (only F16, F32, F64)
2. **Hardware requirement**: Needs H100/newer GPUs for FP8 tensor cores
3. **Complexity**: Requires careful scaling and loss prevention

## ‚úÖ Implemented: Other Memory Techniques

### 1. Gradient Accumulation
**Location**: `config.grad_accum_steps`

**What it does**: Accumulate gradients over N steps before optimizer update

**Memory savings**: Effectively increases batch size without increasing peak memory
```
Real batch = batch_size √ó grad_accum_steps
Memory usage = batch_size (not real batch)
```

### 2. Checkpoint Saving (Memory-Efficient)
**Location**: `checkpoint.rs`

**What it does**: Only keep last N checkpoints
```rust
keep_last_checkpoints: 3  // Delete old checkpoints
```

### 3. Weight Tying
**Location**: `config.weight_tied`

**What it does**: Share embedding and LM head weights
```
Savings = vocab_size √ó dim √ó 4 bytes
For 50K vocab, 2048 dim: ~400MB saved
```

## ‚ö†Ô∏è Planned But Not Implemented

### 1. GaLore2 (Low-Rank Gradient Projection)
**Config flag exists**: `use_galore`  
**Status**: Not implemented  
**File exists**: `optim/galore2.rs` (likely stub)

**Would save**: 90% gradient memory for large matrices
```
Instead of storing full gradient (M√óN):
  Store: P (M√ór) + Q (r√óN) where r << min(M,N)
```

### 2. CPU Offloading
**Status**: Not implemented

**Would save**: Move optimizer states to CPU RAM
**Tradeoff**: 10-20% slower due to PCIe transfer

### 3. Activation Checkpointing
**Status**: Not implemented

**Would save**: Recompute activations during backward instead of storing
**Tradeoff**: 30% slower, 50% less memory

## üìä Memory Breakdown (Example: d20-e3-full, 282M params)

### Without 8-bit Optimizer (FP32 everything)
```
Model weights:     282M √ó 4 bytes = 1.1 GB
Optimizer states:  282M √ó 8 bytes = 2.2 GB (momentum + variance)
Activations:       ~4-8 GB (batch dependent)
Gradients:         282M √ó 4 bytes = 1.1 GB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:             ~8-11 GB minimum
Peak:              ~20-30 GB with batch size 4
```

### With 8-bit Optimizer
```
Model weights:     282M √ó 4 bytes = 1.1 GB
Optimizer states:  282M √ó 1 bytes = 0.3 GB (INT8 + scales)
Activations:       ~4-8 GB
Gradients:         282M √ó 4 bytes = 1.1 GB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:             ~6-8 GB minimum
Peak:              ~18-26 GB with batch size 4
```

**Savings**: ~2 GB (10% of peak memory)

### If We Had FP8 Activations (Hypothetical)
```
Model weights:     282M √ó 4 bytes = 1.1 GB
Optimizer states:  282M √ó 1 bytes = 0.3 GB
Activations:       ~2-4 GB (FP8)
Gradients:         282M √ó 2 bytes = 0.6 GB (FP16)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:             ~4-6 GB minimum
Peak:              ~12-16 GB with batch size 4
```

**Potential savings**: ~40% total memory

## üéØ Recommendations

### For Current 24GB GPU (RTX 4090)

**Already Using:**
- ‚úÖ Gradient accumulation (effective larger batches)
- ‚úÖ Weight tying (where applicable)

**Should Enable:**
- üîß 8-bit Muon optimizer (`use_8bit_optim: true`)
  - Test with d20-e3-full config
  - Benchmark memory usage
  - Verify convergence parity

**Can't Use (Hardware Limitation):**
- ‚ùå FP8 (needs H100)
- ‚ùå Large models >300M params (need 48GB+ even with 8-bit)

### For Future 96GB GPU (RTX PRO 6000 Ada)

**Priority**:
1. Enable 8-bit optimizer for all large models
2. Implement GaLore2 for memory-constrained scenarios
3. Consider activation checkpointing for >10B models

## Summary

**What We Have:**
- ‚úÖ 8-bit Muon (75% optimizer memory reduction)
- ‚úÖ Gradient accumulation
- ‚úÖ Weight tying
- ‚ö†Ô∏è GaLore2 (flag exists, not implemented)

**What We Don't Have:**
- ‚ùå FP8 training (not supported by Candle)
- ‚ùå Activation checkpointing
- ‚ùå CPU offloading

**Current Status:**
- 8-bit optimizer code exists but **NOT VALIDATED**
- Need to benchmark memory usage and convergence
- Should test on larger models to see real impact
